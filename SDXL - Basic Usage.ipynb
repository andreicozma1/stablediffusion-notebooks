{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage\n",
    "\n",
    "- https://huggingface.co/docs/diffusers/main/en/using-diffusers/sdxl\n",
    "- https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/stable_diffusion_xl\n",
    " \n",
    "Pipeline Source Code: https://github.com/huggingface/diffusers/tree/main/src/diffusers/pipelines/stable_diffusion_xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import diffusers\n",
    "\n",
    "from src import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18780d75ed3b4548aba696a5760bcf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StableDiffusionXLPipeline {\n",
      "  \"_class_name\": \"StableDiffusionXLPipeline\",\n",
      "  \"_diffusers_version\": \"0.20.2\",\n",
      "  \"_name_or_path\": \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
      "  \"force_zeros_for_empty_prompt\": true,\n",
      "  \"scheduler\": [\n",
      "    \"diffusers\",\n",
      "    \"EulerDiscreteScheduler\"\n",
      "  ],\n",
      "  \"text_encoder\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTextModel\"\n",
      "  ],\n",
      "  \"text_encoder_2\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTextModelWithProjection\"\n",
      "  ],\n",
      "  \"tokenizer\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTokenizer\"\n",
      "  ],\n",
      "  \"tokenizer_2\": [\n",
      "    \"transformers\",\n",
      "    \"CLIPTokenizer\"\n",
      "  ],\n",
      "  \"unet\": [\n",
      "    \"diffusers\",\n",
      "    \"UNet2DConditionModel\"\n",
      "  ],\n",
      "  \"vae\": [\n",
      "    \"diffusers\",\n",
      "    \"AutoencoderKL\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "model_id = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "refiner_id = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n",
    "\n",
    "pipe = diffusers.StableDiffusionXLPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    "    # low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "pipe.enable_sequential_cpu_offload()\n",
    "\n",
    "print(pipe)\n",
    "print(\"Device:\", pipe.device)\n",
    "\n",
    "# When using torch >= 2.0, you can improve the inference speed by 20-30% with torch.compile. \n",
    "# Simple wrap the unet with torch compile before running the pipeline:\n",
    "# if torch.__version__ >= \"2.0\":\n",
    "#     print(\"Using torch.compile. The first run will be slow, but subsequent runs will be faster.\")\n",
    "#     pipe.unet = torch.compile(pipe.unet, mode=\"reduce-overhead\", fullgraph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22e27cea2c0148e7af826f51c91eed9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 6.00 GiB (GPU 0; 15.69 GiB total capacity; 10.82 GiB already allocated; 286.75 MiB free; 12.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL - Basic Usage.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUsing seed: \u001b[39m\u001b[39m{\u001b[39;00mseed\u001b[39m.\u001b[39mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     generator \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mGenerator(device\u001b[39m=\u001b[39mpipe\u001b[39m.\u001b[39mdevice)\u001b[39m.\u001b[39mmanual_seed(seed\u001b[39m.\u001b[39mvalue)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m out \u001b[39m=\u001b[39m pipe(\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     negative_prompt\u001b[39m=\u001b[39;49mnegative_prompt,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     guidance_scale\u001b[39m=\u001b[39;49mguidance_scale,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     num_inference_steps\u001b[39m=\u001b[39;49mnum_inference_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     num_images_per_prompt\u001b[39m=\u001b[39;49mnum_images_per_prompt,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     generator\u001b[39m=\u001b[39;49mgenerator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/mnt/Documents/GitHub/research/stablediffusion-explained/SDXL%20-%20Basic%20Usage.ipynb#W5sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m helpers\u001b[39m.\u001b[39mplot(out\u001b[39m.\u001b[39mimages)\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/diffusers/pipelines/stable_diffusion_xl/pipeline_stable_diffusion_xl.py:845\u001b[0m, in \u001b[0;36mStableDiffusionXLPipeline.__call__\u001b[0;34m(self, prompt, prompt_2, height, width, num_inference_steps, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, output_type, return_dict, callback, callback_steps, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size)\u001b[0m\n\u001b[1;32m    842\u001b[0m     latents \u001b[39m=\u001b[39m latents\u001b[39m.\u001b[39mto(\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvae\u001b[39m.\u001b[39mpost_quant_conv\u001b[39m.\u001b[39mparameters()))\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    844\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m output_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlatent\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 845\u001b[0m     image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvae\u001b[39m.\u001b[39;49mdecode(latents \u001b[39m/\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvae\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mscaling_factor, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    846\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    847\u001b[0m     image \u001b[39m=\u001b[39m latents\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_hf_hook\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook, \u001b[39m\"\u001b[39m\u001b[39mpre_forward\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpre_forward(\u001b[39mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:270\u001b[0m, in \u001b[0;36mAutoencoderKL.decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    268\u001b[0m     decoded \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(decoded_slices)\n\u001b[1;32m    269\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 270\u001b[0m     decoded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_decode(z)\u001b[39m.\u001b[39msample\n\u001b[1;32m    272\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m    273\u001b[0m     \u001b[39mreturn\u001b[39;00m (decoded,)\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/diffusers/models/autoencoder_kl.py:257\u001b[0m, in \u001b[0;36mAutoencoderKL._decode\u001b[0;34m(self, z, return_dict)\u001b[0m\n\u001b[1;32m    254\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiled_decode(z, return_dict\u001b[39m=\u001b[39mreturn_dict)\n\u001b[1;32m    256\u001b[0m z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpost_quant_conv(z)\n\u001b[0;32m--> 257\u001b[0m dec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(z)\n\u001b[1;32m    259\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m    260\u001b[0m     \u001b[39mreturn\u001b[39;00m (dec,)\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/diffusers/models/vae.py:271\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, z, latent_embeds)\u001b[0m\n\u001b[1;32m    269\u001b[0m     \u001b[39m# up\u001b[39;00m\n\u001b[1;32m    270\u001b[0m     \u001b[39mfor\u001b[39;00m up_block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mup_blocks:\n\u001b[0;32m--> 271\u001b[0m         sample \u001b[39m=\u001b[39m up_block(sample, latent_embeds)\n\u001b[1;32m    273\u001b[0m \u001b[39m# post-process\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[39mif\u001b[39;00m latent_embeds \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:2334\u001b[0m, in \u001b[0;36mUpDecoderBlock2D.forward\u001b[0;34m(self, hidden_states, temb)\u001b[0m\n\u001b[1;32m   2332\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2333\u001b[0m     \u001b[39mfor\u001b[39;00m upsampler \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsamplers:\n\u001b[0;32m-> 2334\u001b[0m         hidden_states \u001b[39m=\u001b[39m upsampler(hidden_states)\n\u001b[1;32m   2336\u001b[0m \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/diffusers/models/resnet.py:169\u001b[0m, in \u001b[0;36mUpsample2D.forward\u001b[0;34m(self, hidden_states, output_size)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_conv:\n\u001b[1;32m    168\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mconv\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m         hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv(hidden_states)\n\u001b[1;32m    170\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m         hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mConv2d_0(hidden_states)\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/Documents/GitHub/research/stablediffusion-explained/.venv/lib/python3.10/site-packages/diffusers/models/lora.py:96\u001b[0m, in \u001b[0;36mLoRACompatibleConv.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_layer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m         \u001b[39m# make sure to the functional Conv2D function as otherwise torch.compile's graph will break\u001b[39;00m\n\u001b[1;32m     95\u001b[0m         \u001b[39m# see: https://github.com/huggingface/diffusers/pull/4315\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m         \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n\u001b[1;32m     97\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     98\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mforward(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlora_layer(x)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 6.00 GiB (GPU 0; 15.69 GiB total capacity; 10.82 GiB already allocated; 286.75 MiB free; 12.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# == The prompt or prompts to guide the image generation. \n",
    "#    If not defined, one has to pass `prompt_embeds` instead.\n",
    "prompt = \"flock of sheep are having selfie with a grazing on grassland, himalayan background extra detailed, highly realistic, extra detailed, himalayn landscape, hyper realistic\"\n",
    "\n",
    "# == The prompt or prompts not to guide the image generation. \n",
    "#    If not defined, one has to pass `negative_prompt_embeds` instead.\n",
    "#    Ignored when not using guidance (i.e., ignored if `guidance_scale` is less than `1`).\n",
    "negative_prompt = \"low-res, low quality, jpeg artifacts, blurry, grainy, distorted, ugly, out of frame, watermarked\"\n",
    "\n",
    "# == Other parameters\n",
    "# == Guidance scale as defined in [Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598).\n",
    "#    `guidance_scale` is defined as `w` of equation 2. of [Imagen Paper](https://arxiv.org/pdf/2205.11487.pdf).\n",
    "#    Guidance scale is enabled by setting `guidance_scale > 1`.\n",
    "#    Higher guidance scale encourages to generate images that are closely linked to the text `prompt`, usually at the expense of lower image quality.\n",
    "guidance_scale = 7.5 # Default: 7.5\n",
    "\n",
    "# == The number of denoising steps.\n",
    "#    More denoising steps usually lead to a higher quality image at the expense of slower inference.\n",
    "num_inference_steps = 30  # Default: 50\n",
    "\n",
    "# == The number of images to generate per prompt.\n",
    "num_images_per_prompt = 6  # Default: 1\n",
    "\n",
    "# ================================================================================================\n",
    "seed = None\n",
    "\n",
    "# == For deterministic results across runs, we create a torch.Generator using the seed value.\n",
    "generator = None\n",
    "if seed is not None:\n",
    "    print(f\"Using seed: {seed.value}\")\n",
    "    generator = torch.Generator(device=pipe.device).manual_seed(seed.value)\n",
    "\n",
    "out = pipe(\n",
    "    prompt=prompt,\n",
    "    negative_prompt=negative_prompt,\n",
    "    guidance_scale=guidance_scale,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    num_images_per_prompt=num_images_per_prompt,\n",
    "    generator=generator,\n",
    ")\n",
    "helpers.plot(out.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refiner Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refiner = diffusers.StableDiffusionXLImg2ImgPipeline.from_pretrained(\n",
    "    refiner_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    use_safetensors=True,\n",
    "    variant=\"fp16\"\n",
    "    # low_cpu_mem_usage=True,\n",
    ").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(pipe)\n",
    "print(\"Device:\", pipe.device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
